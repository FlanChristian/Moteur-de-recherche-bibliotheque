\documentclass[11pt, french]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{float}

\geometry{margin=2.5cm}

% Configuration des listings
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false
}

% En-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\rhead{DAAR - Projet 3}
\lhead{BiblioSearch}
\cfoot{\thepage}


% -------------------- TITRE --------------------
\begin{document}
\begin{titlepage}
    \centering
    \vspace*{1cm}
    \includegraphics[height=3cm]{logo_sorbonne_universite_0.png}\par\vspace{0.6cm}
    \rule{\linewidth}{0.5mm} \\[0.4cm]
    {\huge Rapport du projet DAAR \\[0.4cm]
    \textbf{Conception d’une application web/mobile de recherche avancée dans une bibliothèque numérique}}\\[0.4cm]
    \rule{\linewidth}{0.5mm} \\[1.2cm]
    \textbf{\Large Auteurs :}\\[0.4cm]
    Hamid Zibouche — 21414794\\
    Bih FLAN — 21414764\\
    Awwal FAGBEHOURO - 21417088\\[1.0cm]
    \textbf{\Large Encadrant :}\\[0.3cm]
    BM Bui-Xuan\\[1.0cm]
    {\Large \today}
\end{titlepage}


\newpage
\tableofcontents
\newpage

\section{Introduction}


Dans ce projet, nous allons nous intéresser à la problématique de la recherche et du filtrage
d'informations dans des bibliothèques numériques de grande taille. La quantité croissante de documents
disponibles en ligne rend en effet indispensable la mise en place d’outils performants capables
d’explorer efficacement ces corpus. Le projet que nous nommerons \textit{BiblioSearch} répond à ce besoin en proposant
un moteur de recherche adapté aux bibliothèques numériques, combinant indexation avancée et
analyse de graphes pour améliorer la pertinence des résultats.

Avec plus de 60\,000 livres accessibles sur le \textit{Projet Gutenberg}, il est évident que
la recherche manuelle devient impossible. L’utilisateur doit pouvoir accéder rapidement à un document
par mot-clé, expression régulière ou encore par suggestion de textes similaires. Notre moteur de recherche
s’appuie sur des algorithmes d’indexation performants et sur des techniques issues de la théorie des graphes,
afin de proposer une expérience de recherche intelligente, pertinente et évolutive.



Nos objectifs principaux sont les suivants :
\begin{itemize}[itemsep=5pt]
    \item Construire une bibliothèque de plus de 1726 livres, chacun contenant au moins 10\,000 mots,
    afin de disposer d'un corpus représentatif et suffisamment volumineux.
    \item Implémenter un moteur de recherche explicite par mot-clé et une recherche avancée par
    expressions régulières, permettant une flexibilité accrue.
    \item Développer un système de classement des résultats basé sur des mesures de centralité
    dans un graphe de similarité, afin d’améliorer la pertinence des réponses.
    \item Créer un graphe de similarité fondé sur l’indice de Jaccard pour recommander des livres
    proches ou complémentaires.
    \item Fournir une interface web moderne, responsive et intuitive, facilitant l’interaction
    entre l’utilisateur et la bibliothèque numérique.
\end{itemize}

Pour atteindre ces objectifs, nous avons retenu une pile technologique adaptée aux besoins
de performance et de maintenabilité. Le tableau ci-dessous présente les principaux composants
et les technologies utilisées dans le cadre du projet :

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Composant} & \textbf{Technologie} \\
\midrule
Backend & Django 5.2.8 (Python 3.13) \\
Base de données & PostgreSQL 16 avec extension pg\_trgm \\
Frontend & HTML5, CSS3, JavaScript Vanilla \\
Visualisation & Vis.js 9.1.2, Chart.js 4.4.0 \\
Source des livres & Projet Gutenberg via API Gutendex \\
\bottomrule
\end{tabular}
\caption{Stack technique du projet}
\end{table}

\section{Architecture Générale}

\subsection{Structure de la base de données}

Pour assurer la robustesse et la performance du projet, nous avons choisi \textbf{PostgreSQL} comme système de gestion de base de données. Ce choix se justifie par plusieurs raisons : PostgreSQL est un SGBD relationnel open-source reconnu pour sa stabilité, sa conformité aux standards SQL, et ses extensions puissantes pour la recherche textuelle. En particulier, l’extension \texttt{pg\_trgm} permet d’effectuer des recherches de similarité sur des chaînes de caractères, ce qui est particulièrement adapté à notre problématique de moteur de recherche.

Le schéma de la base de données est conçu pour séparer clairement les responsabilités et optimiser les opérations de recherche et d’analyse. Il est organisé autour de plusieurs tables principales :

\begin{enumerate}
    \item \textbf{books} : contient les métadonnées des livres (titre, auteur, langue, nombre de mots). Cette table sert de point d’entrée pour identifier chaque document.
    \item \textbf{book\_texts} : stocke le contenu textuel intégral des livres. Elle permet de conserver la source brute pour les recherches avancées (par expressions régulières).
    \item \textbf{words} : représente le vocabulaire global, c’est-à-dire l’ensemble des mots uniques rencontrés dans la bibliothèque. Elle facilite la gestion des identifiants de mots.
    \item \textbf{postings} : implémente l’index inversé, en reliant chaque mot (\texttt{word\_id}) aux livres (\texttt{book\_id}) où il apparaît, avec sa fréquence. Cette structure est essentielle pour les recherches rapides par mot-clé.
    \item \textbf{top\_terms} : conserve pour chaque livre les 50 mots les plus significatifs (après suppression des stop words). Cette table est utilisée pour les calculs de similarité et pour alléger les comparaisons entre documents.
    \item \textbf{jaccard\_edges} : encode le graphe de similarité entre livres, basé sur l’indice de Jaccard. Chaque arête relie deux livres et indique leur degré de similarité.
    \item \textbf{book\_centrality} : stocke les métriques de centralité calculées sur le graphe (PageRank, Closeness, Betweenness). Ces valeurs sont utilisées pour classer les résultats de recherche selon leur importance dans le réseau documentaire.
\end{enumerate}

Cette organisation permet de répondre efficacement aux différentes fonctionnalités du projet : recherche rapide par mot-clé, recherche avancée par RegEx, calcul de similarité entre documents, et classement par centralité.

\subsection{Architecture applicative}

L’application web repose sur le framework \textbf{Django}, qui suit le pattern \textbf{MVC} (Modèle–Vue–Contrôleur). Ce choix est motivé par la clarté de séparation des responsabilités et la richesse de l’écosystème Django, qui facilite le développement rapide d’applications robustes.

\begin{itemize}
    \item \textbf{Modèles} : définis via l’ORM Django, ils correspondent directement aux tables de la base de données (Book, BookText, Word, Posting, etc.). Cette couche permet de manipuler les données de manière abstraite et sécurisée, sans écrire directement du SQL.
    \item \textbf{Vues} : elles contiennent la logique applicative, notamment les algorithmes de recherche, de tri et de recommandation. Les vues orchestrent les requêtes vers la base et préparent les résultats pour l’affichage.
    \item \textbf{Templates} : ce sont les pages HTML enrichies de CSS et de JavaScript. Elles assurent l’interactivité côté client, par exemple l’affichage dynamique des résultats, la mise en évidence des mots-clés trouvés, ou la visualisation des graphes de similarité.
\end{itemize}

Cette architecture garantit une bonne modularité : la base de données gère la persistance et l’efficacité des recherches, les modèles assurent une abstraction propre, les vues encapsulent la logique métier, et les templates offrent une interface utilisateur claire et intuitive. En combinant PostgreSQL et Django, nous obtenons une solution à la fois performante, extensible et adaptée aux besoins d’un moteur de recherche de bibliothèque numérique.


\section{Algorithmes Implémentés}

\subsection{Construction de l'Index Inversé}

Un index inversé est une structure de données fondamentale des moteurs de recherche modernes. 
Il associe chaque mot du vocabulaire à la liste des documents dans lesquels ce mot apparaît, 
ainsi qu’à sa fréquence d’occurrence. Cette organisation permet de retrouver rapidement 
tous les documents pertinents pour un mot-clé donné, sans avoir à parcourir l’ensemble du corpus. 
Formellement, pour chaque mot $w$, on maintient une structure de la forme :

\begin{equation}
    \text{Index}[w] = \{(d_1, f_1), (d_2, f_2), \ldots, (d_n, f_n)\}
\end{equation}

où $d_i$ désigne un document et $f_i$ la fréquence du mot $w$ dans ce document.

La construction de l’index inversé repose sur un processus en trois étapes. 
Tout d’abord, chaque fichier est lu et son contenu est \textbf{tokenisé}, c’est-à-dire découpé en unités lexicales 
après normalisation (mise en minuscule, suppression de la ponctuation) et filtrage des mots vides 
(\textit{stop words}) qui n’apportent pas de valeur sémantique. Ensuite, les fréquences des mots 
sont calculées pour chaque document, ce qui permet de quantifier l’importance relative de chaque terme. 
Enfin, ces informations sont insérées dans la base de données, en reliant chaque mot à son identifiant 
et au document concerné. Le code suivant illustre ce processus :

\begin{lstlisting}[language=Python, caption=Algorithme d'indexation]
def ingest_file(cur, path):
    # Lecture et tokenisation
    text = read_file(path)
    tokens = tokenize(text)  # Normalisation + suppression des stopwords
    
    # Comptage des fréquences
    counts = Counter(tokens)
    
    # Insertion dans la base
    for word, count in counts.items():
        word_id = get_or_create_word(word)
        insert_posting(word_id, book_id, count)
\end{lstlisting}

Afin d’améliorer les performances et de garantir la scalabilité du système, plusieurs optimisations ont été mises en place :

\begin{itemize}
    \item \textbf{Bulk insert} : utilisation de la fonction \texttt{execute\_values} pour insérer les données par paquets, ce qui réduit considérablement le temps d’écriture par rapport à des insertions individuelles.
    \item \textbf{Stop words} : filtrage de 179 mots courants (par exemple « the », « a », « of ») afin d’éviter que ces termes ne saturent l’index et ne biaisent les calculs de pertinence.
    \item \textbf{Index B-tree} : création d’un index sur la colonne \texttt{words.w}, permettant une recherche rapide en temps logarithmique ($O(\log n)$).
    \item \textbf{Top-50 terms} : stockage séparé des 50 mots les plus significatifs par livre, ce qui accélère les calculs de similarité et les recherches prioritaires.
\end{itemize}

Grâce à cette organisation, l’index inversé constitue la pierre angulaire du moteur de recherche : 
il offre une structure compacte et efficace pour relier vocabulaire et documents, 
tout en permettant des optimisations adaptées aux volumes importants de données.


\subsection{Recherche par Mot-clé}

La recherche par mot-clé dans notre bibliothèque est l'objesctif de notre projet. Pour cela, nous avons choisi de mettre en place
une recherche hiérarchisée à trois niveaux de priorité :

\begin{enumerate}
    \item \textbf{Priorité 0} : Recherche dans les \textit{titres} des livres. Les titres contiennent souvent le sujet principal
    et permettent d’obtenir rapidement des résultats très pertinents.
    \item \textbf{Priorité 1} : Recherche dans les \textit{top-50 terms}, c’est-à-dire les mots les plus significatifs de chaque livre
    (après suppression des stop words). Ces termes capturent l’essence thématique du document.
    \item \textbf{Priorité 2} : Recherche dans l’index complet (\textit{postings}), garantissant l’exhaustivité en couvrant
    l’ensemble des mots présents dans la bibliothèque.
\end{enumerate}

Cette hiérarchie est justifiée par un compromis entre rapidité et exhaustivité : les titres et les termes représentatifs
permettent de cibler rapidement les documents les plus pertinents, tandis que l’index complet assure que
tous les cas sont couverts, même si le mot-clé est rare ou secondaire.

L’algorithme de recherche combine ces trois niveaux au sein d’une requête SQL unifiée. 
Chaque sous-requête identifie les documents correspondant à un niveau de priorité, puis les résultats sont fusionnés
dans une table temporaire. Enfin, les documents sont triés selon leur priorité, puis par pertinence mesurée
à l’aide de métriques comme le \textit{pagerank} ou le nombre d’occurrences du mot-clé.

\begin{lstlisting}[language=SQL, caption=Requête SQL de recherche hiérarchisée]
WITH all_matches AS (
    -- Recherche dans titres (priorité 0)
    SELECT b.*, 'title' AS source, 0 AS priority
    FROM books b
    WHERE LOWER(b.title) LIKE '%' || query || '%'
    
    UNION ALL
    
    -- Recherche dans top_terms (priorité 1)
    SELECT b.*, 'top_terms' AS source, 1 AS priority
    FROM top_terms tt
    JOIN books b ON b.id = tt.book_id
    WHERE LOWER(tt.w) = query
    
    UNION ALL
    
    -- Recherche dans postings (priorité 2)
    SELECT b.*, 'postings' AS source, 2 AS priority
    FROM words w
    JOIN postings p ON p.word_id = w.id
    JOIN books b ON b.id = p.book_id
    WHERE LOWER(w.w) = query
)
SELECT DISTINCT * FROM all_matches
ORDER BY priority, pagerank DESC, occurrences DESC;
\end{lstlisting}


\subsection{Recherche Avancée par RegEx}

Tout RegEx peut être représentée
par un \textbf{automate fini non déterministe} (NFA). Cet automate décrit l’ensemble des transitions possibles
pour reconnaître un motif dans un texte. Cependant, l’exécution directe d’un NFA peut être coûteuse, car il faut
explorer simultanément plusieurs chemins. Pour optimiser la recherche, on applique la \textbf{construction de
subset} qui transforme le NFA en un \textbf{automate fini déterministe} (DFA). Le DFA ne possède qu’un seul état
actif à chaque étape, ce qui rend la reconnaissance beaucoup plus efficace. Dans notre moteur, les RegEx saisies par l’utilisateur sont compilées en NFA puis converties en DFA avant
d’être exécutées sur les documents candidats identifiés par l’index inversé. Ainsi, la recherche avancée par RegEx reste expressive tout en garantissant
des temps de réponse raisonnables, même sur un corpus de plusieurs milliers de livres.


\subsection{Graphe de Jaccard}

Pour les fonctionnalités implicites telles que le classement et la suggestion, nous allons construire un graphe de similarité entre les documents. Dans notre projet, nous avons choisi
l’indice de Jaccard comme mesure de similarité entre livres, car il est simple à calculer et offre une interprétation
intuitive : plus deux ensembles de mots partagent d’éléments, plus ils sont considérés comme proches.

La similarité de Jaccard mesure le chevauchement entre deux ensembles de termes. Elle est définie par la formule :

\begin{equation}
    J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
\end{equation}

La distance de Jaccard, complémentaire, est donnée par :

\begin{equation}
    d_J(A, B) = 1 - J(A, B)
\end{equation}

Ainsi, deux documents sont d’autant plus similaires que leur intersection de vocabulaire est grande relativement
à leur union. Dans notre moteur de recherche, chaque livre est représenté par un ensemble de ses mots les plus significatifs.
Plutôt que d’utiliser l’intégralité du vocabulaire, ce qui serait coûteux et peu discriminant, nous retenons
les \textbf{top-50 termes} de chaque document. Ces termes sont sélectionnés après suppression des stop words
et reflètent l’essence thématique du livre.  

Pour deux livres $L_1$ et $L_2$, on définit donc :
\begin{itemize}
    \item $V_1$ = ensemble des top-50 mots de $L_1$
    \item $V_2$ = ensemble des top-50 mots de $L_2$
\end{itemize}

Ce choix permet de réduire la complexité tout en conservant une bonne qualité de mesure. Par exemple,
pour un corpus de 1664 livres, on obtient 
$1664 \times 1663 / 2 = 1\,383\,616$ paires à comparer, 
ce qui illustre bien l’importance de limiter la taille des ensembles de mots utilisés 
afin de maintenir la faisabilité des calculs.

\bigskip
L’algorithme de construction du graphe de Jaccard parcourt toutes les paires de livres, calcule leur similarité
et insère une arête dans le graphe si la valeur dépasse un seuil fixé (ici 0.5). Ce seuil garantit que seules
les relations significatives sont conservées, évitant ainsi un graphe trop dense et peu exploitable.

\begin{lstlisting}[language=Python, caption=Construction du graphe de Jaccard]
def build_jaccard(conn):
    docs = load_top_terms(conn)  # {book_id: {word_id: cnt}}
    
    edges = []
    for (b1, b2) in combinations(book_ids, 2):
        # Intersection et union
        common = set(docs[b1].keys()) & set(docs[b2].keys())
        union = set(docs[b1].keys()) | set(docs[b2].keys())
        
        # Similarité
        similarity = len(common) / len(union) if union else 0
        
        # Seuil 0.5 : au moins 50% de mots communs
        if similarity >= 0.5:
            edges.append((b1, b2, similarity))
    
    # Insertion dans jaccard_edges
    insert_edges(conn, edges)
\end{lstlisting}

Le graphe obtenu est ensuite utilisé pour calculer des mesures de centralité (PageRank, Closeness, Betweenness),
et pour générer des suggestions de documents similaires. Il constitue donc une structure pivot reliant
les fonctionnalités explicites (recherche) aux fonctionnalités implicites (classement et recommandation).


\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Métrique} & \textbf{Valeur} \\
\midrule
Livres dans le graphe & 1726 \\
Paires possibles & 1~489~225 \\
Arêtes créées (seuil 0.5) & 47~143 \\
Densité du graphe & 3.17\% \\
Similarité minimale & 0.5001 \\
Similarité moyenne & 0.5523 \\
Similarité maximale & 0.8012 \\
\bottomrule
\end{tabular}
\caption{Statistiques du graphe de Jaccard}
\end{table}

\subsection{Métriques de Centralité}

Les métriques de centralité permettent de mesurer l’importance relative des nœuds dans un graphe. 
Dans notre projet, elles sont appliquées au graphe de similarité de Jaccard entre livres. 
L’idée est que certains documents jouent un rôle plus central que d’autres : 
ils peuvent être fortement connectés à de nombreux livres (PageRank), 
être proches de tous les autres (Closeness), ou encore servir de passage obligé 
dans de nombreux chemins (Betweenness). Ces mesures enrichissent la recherche 
en permettant de classer les résultats selon des critères structurels 
et non uniquement lexicaux.

\subsubsection{PageRank}

Le \textbf{PageRank} mesure l’importance d’un nœud en propageant itérativement les scores 
à travers le graphe. Un livre est considéré comme important s’il est relié à d’autres livres 
eux-mêmes importants, avec des poids d’arêtes reflétant la force de la similarité de Jaccard.

\begin{equation}
    PR(v) = \frac{1-d}{N} + d \sum_{u \in N(v)} \frac{PR(u) \cdot w(u,v)}{\sum_{w \in N(u)} w(u,w)}
\end{equation}

où $d=0.85$ est le facteur d’amortissement, $N$ le nombre total de livres, 
et $w(u,v)$ le poids de l’arête entre $u$ et $v$.  

L’algorithme repose sur une méthode d’itérations successives (\textit{power iteration}) 
jusqu’à convergence :

\begin{lstlisting}[language=Python, caption=Algorithme PageRank]
def compute_pagerank(adjacency, book_ids, damping=0.85):
    n = len(book_ids)
    pagerank = {book: 1.0/n for book in book_ids}
    
    for iteration in range(100):
        new_pr = {}
        diff = 0
        
        for book in book_ids:
            rank_sum = 0
            for neighbor in book_ids:
                if book in adjacency[neighbor]:
                    weight = adjacency[neighbor][book]
                    out_weight = sum(adjacency[neighbor].values())
                    rank_sum += pagerank[neighbor] * weight / out_weight
            
            new_pr[book] = (1-damping)/n + damping * rank_sum
            diff += abs(new_pr[book] - pagerank[book])
        
        pagerank = new_pr
        if diff < 1e-6:  # Convergence
            break
    
    return pagerank
\end{lstlisting}

Dans notre corpus, la convergence est atteinte en environ 50 itérations avec $\epsilon = 10^{-6}$.

\subsubsection{Closeness Centrality}

La \textbf{Closeness Centrality} mesure la proximité moyenne d’un nœud à tous les autres. 
Un livre est considéré comme central s’il peut atteindre rapidement tous les autres 
dans le graphe de similarité. Cette mesure est utile pour identifier des documents 
qui servent de “ponts thématiques” entre différentes parties de la bibliothèque.

\begin{equation}
    C(v) = \frac{n-1}{\sum_{u \neq v} d(v,u)}
\end{equation}

où $d(v,u)$ est la distance (plus court chemin) entre $v$ et $u$.  
L’algorithme repose sur un parcours en largeur (BFS) depuis chaque nœud :

\begin{lstlisting}[language=Python, caption=Algorithme Closeness]
def compute_closeness(adjacency, book_ids):
    closeness = {}
    
    for source in book_ids:
        # BFS pour calculer les distances
        distances = bfs_distances(adjacency, source)
        
        # Closeness = 1 / distance moyenne
        if len(distances) > 1:
            avg_dist = sum(distances.values()) / (len(distances)-1)
            closeness[source] = 1.0 / avg_dist if avg_dist > 0 else 0
        else:
            closeness[source] = 0
    
    return closeness
\end{lstlisting}

La complexité est $O(|V| \cdot (|V| + |E|))$, ce qui reste raisonnable pour des graphes de taille moyenne.

\subsubsection{Betweenness Centrality}

La \textbf{Betweenness Centrality} mesure le nombre de plus courts chemins passant par un nœud. 
Un livre est considéré comme central s’il apparaît fréquemment dans les chemins reliant 
d’autres livres. Cette métrique met en évidence des documents “charnières” qui relient 
différentes communautés thématiques.

\begin{equation}
    B(v) = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}
\end{equation}

où $\sigma_{st}$ est le nombre de plus courts chemins entre $s$ et $t$, 
et $\sigma_{st}(v)$ le nombre de ces chemins passant par $v$.  

L’algorithme de Brandes (2001) permet un calcul efficace en $O(|V| \cdot |E|)$ :

\begin{lstlisting}[language=Python, caption=Algorithme de Brandes]
def compute_betweenness(adjacency, book_ids):
    betweenness = {book: 0.0 for book in book_ids}
    
    for source in book_ids:
        stack, pred, sigma, dist = [], {}, {}, {}
        
        # Phase 1: BFS + comptage des chemins
        queue = [source]
        sigma[source] = 1
        dist[source] = 0
        
        while queue:
            v = queue.pop(0)
            stack.append(v)
            for w in adjacency.get(v, {}):
                if w not in dist:  # Découverte
                    queue.append(w)
                    dist[w] = dist[v] + 1
                if dist[w] == dist[v] + 1:  # Plus court chemin
                    sigma[w] = sigma.get(w, 0) + sigma[v]
                    pred.setdefault(w, []).append(v)
        
        # Phase 2: Accumulation
        delta = {book: 0.0 for book in book_ids}
        while stack:
            w = stack.pop()
            for v in pred.get(w, []):
                delta[v] += (sigma[v]/sigma[w]) * (1 + delta[w])
            if w != source:
                betweenness[w] += delta[w]
    
    # Normalisation (graphe non orienté)
    n = len(book_ids)
    if n > 2:
        norm = 2.0 / ((n-1) * (n-2))
        betweenness = {k: v*norm for k, v in betweenness.items()}
    
    return betweenness
\end{lstlisting}

Cette métrique est plus coûteuse à calculer que les deux précédentes, 
mais elle apporte une information complémentaire précieuse sur la structure du graphe.

\section{Tests et Validation}

\subsection{Méthodologie de test}

Tous les livres utilisés proviennent du \textbf{Projet Gutenberg}, récupérés via l’API \textbf{Gutendex}.
Nous avons retenu uniquement les ouvrages en anglais, français et allemand contenant au moins 10\,000 mots,
afin de garantir un corpus représentatif et suffisamment volumineux. 

\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Métrique} & \textbf{Valeur} \\
\midrule
Livres indexés & 1726 \\
Langues & 3 (anglais, français, allemand) \\
Mots uniques & 738\,502 \\
Postings (index inversé) & 13\,046\,418 \\
Top terms & 9\,340 \\
Total mots & 72\,074\,153 \\
Arêtes Jaccard & 47\,143 \\
\bottomrule
\end{tabular}
\caption{Statistiques du dataset}
\end{table}

\subsection{Tests de performance}

\subsubsection{Temps de recherche}

Nous avons conduit une série de benchmarks sur le corpus complet afin de mesurer les performances réelles du système.
Chaque test a été répété plusieurs fois pour obtenir des valeurs fiables.  

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Type de recherche} & \textbf{Temps moyen} \\
\midrule
Recherche par titre (LIKE) & 5.4 ms \\
Recherche dans top\_terms & 49.9 ms \\
Recherche dans postings & 378.0 s \\
Recherche RegEx & 150.2 ms \\
Construction Jaccard (1726 livres) & $\sim$45 min \\
Calcul centralité (1726 livres) & $\sim$7 min \\
\bottomrule
\end{tabular}
\caption{Performance des algorithmes (mesures réelles)}
\end{table}

Les résultats montrent une hiérarchie nette : la recherche par titre est quasi instantanée (<5 ms),
les top-terms restent rapides (<50 ms), tandis que l’index complet et les RegEx sont plus coûteux.
Cette différence justifie notre architecture hiérarchisée à trois niveaux de priorité.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{2_scalabilite.png}
\caption{Scalabilité : temps de recherche en fonction de la taille du corpus}
\end{figure}

Les tests de scalabilité confirment que les temps de recherche croissent de manière sous-linéaire,
grâce aux index B-tree de PostgreSQL. Même avec 1726 livres, les requêtes restent sous la seconde,
ce qui valide la robustesse de l’architecture.

\subsubsection{Performance des métriques de centralité}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4_centralite.png}
\caption{Performance des métriques de centralité (Top 100 livres)}
\end{figure}

Les trois métriques de centralité (PageRank, Closeness, Betweenness) sont pré-calculées et stockées
dans la table \texttt{book\_centrality}. Le tri des résultats selon ces critères est donc instantané
(<1 ms pour récupérer le top 100), ce qui permet une expérience utilisateur fluide.

\subsubsection{Requêtes sur le graphe de Jaccard}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{5_jaccard.png}
\caption{Performance des requêtes du graphe de Jaccard}
\end{figure}

Les requêtes sur le graphe de Jaccard (top paires, voisins d’un livre, statistiques globales)
sont très efficaces grâce aux index sur \texttt{jaccard\_edges}. Même la requête la plus coûteuse
(liste des voisins d’un livre) reste sous 100 ms.

\subsubsection{Validation des recommandations Jaccard}

Nous avons validé la pertinence des recommandations générées par la similarité de Jaccard.
Pour le livre \textit{Pride and Prejudice}, les suggestions sont :

\begin{enumerate}
    \item \textit{Emma} (J. Austen) — Similarité : 0.60
    \item \textit{Sense and Sensibility} (J. Austen) — Similarité : 0.67
    \item \textit{Mansfield Park} (J. Austen) — Similarité : 0.61
\end{enumerate}


Toutes ces recommandations appartiennent au même auteur et au même genre littéraire (romans romantiques du XIX\textsuperscript{e} siècle),
ce qui confirme que la métrique de Jaccard capture efficacement les thématiques communes.

\subsubsection{Complexité des RegEx}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{6_regex_complexity.png}
\caption{Impact de la complexité des expressions régulières}
\end{figure}

Les tests montrent que la complexité du motif RegEx influe directement sur les temps de recherche :
les patterns simples (\texttt{\^love}) sont environ deux fois plus rapides que les patterns complexes
(\texttt{[a-z]*love[a-z]*}). Cette observation permet de guider l’utilisateur vers des requêtes optimisées.




\section{Interface Utilisateur}

Nous avons conçu une interface utilisateur simple,
responsive et intuitive, qui met en valeur les fonctionnalités principales du moteur de recherche
tout en offrant une expérience fluide et agréable.

\bigskip

La page d’accueil est le point d’entrée de l’application. Elle propose une \textbf{barre de recherche}
avec autocomplétion, permettant à l’utilisateur de saisir rapidement un mot-clé ou une expression régulière.
Des \textbf{filtres} sont disponibles pour préciser la langue, le type de recherche (simple ou avancée par RegEx),
et le mode de tri des résultats. Une section dédiée aux \textbf{livres populaires} met en avant les documents
ayant obtenu les meilleurs scores de PageRank, ce qui permet de guider l’utilisateur vers les ouvrages
les plus centraux dans le graphe de similarité. Enfin, des \textbf{statistiques en temps réel} 
affichent la taille de la bibliothèque (nombre de livres, langues disponibles, taille de l’index),
afin de donner une vision globale de la richesse du corpus.

\bigskip

Les résultats sont présentés sous forme de \textbf{cartes de livres}, avec les couvertures issues du projet Gutenberg.
Chaque carte est enrichie de \textbf{badges} indiquant la source de la correspondance (titre, top-50 termes ou index complet),
ce qui permet à l’utilisateur de comprendre immédiatement pourquoi un document apparaît dans la liste.
Six options de tri sont proposées : pertinence, PageRank, occurrences du mot-clé, Closeness, Betweenness et ordre alphabétique par titre.
Des \textbf{statistiques de recherche} indiquent le nombre de résultats trouvés par source, ce qui aide à évaluer
la précision et la portée de la requête.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{accueil.png}
\caption{Page d'accueil : barre de recherche, filtres, livres populaires et statistiques en temps réel}
\end{figure}

La page de détail fournit une vue approfondie sur un livre sélectionné. Elle affiche les \textbf{informations complètes}
(titre, auteur, langue, nombre de mots), ainsi que les \textbf{top-10 mots les plus fréquents} accompagnés de leurs compteurs.
Une section dédiée présente les \textbf{livres similaires}, calculés via la similarité de Jaccard, avec leurs scores de proximité.
L’utilisateur peut également explorer les autres ouvrages du même auteur, ou naviguer facilement vers le livre précédent ou suivant.
Cette page combine donc des informations descriptives et analytiques, renforçant la compréhension du contenu et de son contexte.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{graphe_jaccard.png}
\caption{Visualisation interactive du graphe de Jaccard avec 1726 nœuds et 47\,143 arêtes}
\end{figure}

Enfin, une fonctionnalité originale de l'interface est la visualisation du \textbf{graphe de Jaccard} come montré dans la figure 6 grâce à la librairie Vis.js.
Le graphe représente les livres comme des nœuds et leurs similarités comme des arêtes. Dans notre corpus de 1726 livres,
cela correspond à 47\,143 arêtes. L'utilisateur peut ajuster dynamiquement le \textbf{seuil de similarité} (de 0.5 à 1.0)
via un slider, ce qui modifie instantanément la densité du graphe. Des fonctionnalités d'interaction telles que le zoom,
le déplacement et la sélection de nœuds permettent une exploration intuitive. Des statistiques complémentaires
affichent le nombre de nœuds et d'arêtes, la similarité moyenne, ainsi que les \textbf{top 20 paires les plus similaires}
et les \textbf{top 15 livres les plus connectés}. Cette visualisation rend tangible la structure du corpus et met en évidence
les relations thématiques entre les ouvrages.


\section{Conclusion}

Le projet \textit{BiblioSearch} a démontré la pertinence de combiner des techniques classiques de recherche textuelle 
(index inversé, expressions régulières) avec des approches issues de la théorie des graphes (similarité de Jaccard, 
métriques de centralité). Cette intégration permet non seulement d’assurer des recherches rapides et efficaces, 
mais aussi de proposer des recommandations thématiques et un classement intelligent des résultats. 
Avec plus de 1700 livres indexés et une interface moderne, l’application atteint ses objectifs techniques 
tout en restant extensible à des corpus plus vastes.

Au-delà de la réalisation technique, ce projet a eu un impact pédagogique fort. Il nous a permis de mettre en pratique 
des concepts variés allant de l’optimisation de bases de données à l’implémentation d’algorithmes de graphes, 
en passant par le développement web full-stack. Les perspectives d’amélioration, telles que l’intégration de 
méthodes de clustering ou de représentations vectorielles (Word2Vec, BERT), ouvrent la voie vers un moteur 
de recherche encore plus intelligent et adapté aux besoins des utilisateurs.

\bigskip

\section*{Références}

\begin{enumerate}[label={[\arabic*]}]
    \item Mehdi Naima, Lionel Tabourier. \textit{AAGA - Moteurs de recherche : méthode de la puissance itérée}. LIP6 – CNRS and Sorbonne Université.
    
    \item Mehdi Naima, Lionel Tabourier. \textit{Analyse d'Algorithmes, Graphes et Applications - 2. Homophily and Centrality}. LIP6 – CNRS and Sorbonne Université.
    
    \item Django Documentation. \url{https://docs.djangoproject.com/}
    
    \item PostgreSQL Documentation. \url{https://www.postgresql.org/docs/}
    
    \item Gutenberg Project. \url{https://www.gutenberg.org/}
    
    \item Gutendex API. \url{https://gutendex.com/}
\end{enumerate}

\section*{Ressources complémentaires}

Le code source du projet est disponible sur GitHub :

\begin{itemize}
    \item \url{https://github.com/FlanChristian/Moteur-de-recherche-bibliotheque}
\end{itemize}

Cette ressource permet de consulter l’implémentation complète, incluant l’indexation inversée,
les algorithmes de graphes (PageRank, Closeness, Betweenness), ainsi que l’interface web
développée avec Django et Vis.js.


\end{document}